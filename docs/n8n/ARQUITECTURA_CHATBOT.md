# ğŸ—ï¸ Arquitectura del Chatbot Inteligente

DocumentaciÃ³n visual de cÃ³mo funciona el chatbot internamente.

---

## ğŸ“Š Diagrama del Flujo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USUARIO/CLIENTE                              â”‚
â”‚              (EnvÃ­a pregunta vÃ­a API/Webhook)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ POST /webhook/chatbot
                             â”‚ {message: "...", history: [...]}
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1ï¸âƒ£  WEBHOOK (n8n)                                                  â”‚
â”‚  â”œâ”€ Recibe POST request                                             â”‚
â”‚  â”œâ”€ Path: /chatbot                                                  â”‚
â”‚  â”œâ”€ Modo respuesta: Via nodo respondToWebhook                       â”‚
â”‚  â””â”€ Pasa datos al siguiente nodo                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2ï¸âƒ£  EXTRAER CONTEXTO (Code Node)                                   â”‚
â”‚  â”œâ”€ Obtiene: message del usuario                                    â”‚
â”‚  â”œâ”€ Obtiene: history de conversaciÃ³n                                â”‚
â”‚  â”œâ”€ Extrae: projectContext (embedido)                               â”‚
â”‚  â”‚   â””â”€ CaracterÃ­sticas del proyecto                                â”‚
â”‚  â”‚   â””â”€ Stack tecnolÃ³gico                                           â”‚
â”‚  â”‚   â””â”€ Ã‰picas y features                                           â”‚
â”‚  â”‚   â””â”€ Roles del sistema                                           â”‚
â”‚  â”‚   â””â”€ Estructura del proyecto                                     â”‚
â”‚  â”‚   â””â”€ Base de datos                                               â”‚
â”‚  â””â”€ Retorna: {userMessage, conversationHistory, projectContext}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3ï¸âƒ£  CONSTRUIR PROMPT (Code Node)                                   â”‚
â”‚  â”œâ”€ Crea System Prompt:                                             â”‚
â”‚  â”‚   â”œâ”€ Contexto completo del proyecto                              â”‚
â”‚  â”‚   â”œâ”€ Instrucciones de comportamiento                             â”‚
â”‚  â”‚   â””â”€ Restricciones y guidelines                                  â”‚
â”‚  â”œâ”€ Crea User Prompt:                                               â”‚
â”‚  â”‚   â”œâ”€ Historial de conversaciÃ³n (si existe)                       â”‚
â”‚  â”‚   â””â”€ Pregunta actual del usuario                                 â”‚
â”‚  â””â”€ Formatea para LLM en JSON                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4ï¸âƒ£  LLM (OpenAI o Claude)                                          â”‚
â”‚  â”œâ”€ Recibe: [{role: system, content: systemPrompt},                 â”‚
â”‚  â”‚           {role: user, content: userPrompt}]                     â”‚
â”‚  â”œâ”€ ParÃ¡metros:                                                     â”‚
â”‚  â”‚   â”œâ”€ Model: gpt-4-turbo o claude-3-5-sonnet                      â”‚
â”‚  â”‚   â”œâ”€ Temperature: 0.7 (balance)                                  â”‚
â”‚  â”‚   â””â”€ MaxTokens: 1000-1024                                        â”‚
â”‚  â””â”€ Retorna: Respuesta generada por IA                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5ï¸âƒ£  PROCESAR RESPUESTA (Code Node)                                 â”‚
â”‚  â”œâ”€ Extrae contenido de la respuesta LLM                            â”‚
â”‚  â”œâ”€ Agrega timestamp actual                                         â”‚
â”‚  â”œâ”€ Marca como success: true/false                                  â”‚
â”‚  â””â”€ Retorna: {reply, timestamp, success, model}                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6ï¸âƒ£  RESPONDER (Webhook Response Node)                              â”‚
â”‚  â”œâ”€ Formatea respuesta JSON                                         â”‚
â”‚  â”œâ”€ Status HTTP 200 OK                                              â”‚
â”‚  â””â”€ Retorna al cliente                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESPUESTA AL USUARIO                             â”‚
â”‚  {                                                                  â”‚
â”‚    "reply": "El sistema FlotaVehicular es...",                      â”‚
â”‚    "timestamp": "2025-12-17T10:30:00Z",                             â”‚
â”‚    "success": true,                                                 â”‚
â”‚    "model": "GPT-4 Turbo" o "Claude 3.5 Sonnet"                     â”‚
â”‚  }                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Flujo de Datos Detallado

### Stage 1: Ingesta

```
INPUT:
{
  "message": "Â¿CuÃ¡l es la arquitectura?",
  "history": [
    {"role": "user", "content": "Hola"},
    {"role": "assistant", "content": "Hola, soy..."}
  ]
}

VALIDACIÃ“N:
âœ… message: string vÃ¡lido
âœ… history: array vÃ¡lido
```

### Stage 2: PreparaciÃ³n de Contexto

```
PROJECT_CONTEXT = {
  caracterÃ­sticas: [
    "Dashboard Inteligente",
    "GestiÃ³n de Flota",
    "Monitoreo RT",
    ...
  ],
  stack: {
    frontend: ["React 18.3", "TypeScript", "Vite"],
    backend: ["Supabase", "PostgreSQL"],
    otros: ["Google Maps", "GPS"]
  },
  Ã©picas: 10,
  roles: 7,
  base_datos: "Supabase PostgreSQL"
}
```

### Stage 3: ConstrucciÃ³n de Prompt

```
SYSTEM_PROMPT = """
Eres un asistente experto sobre FlotaVehicular...
[Contexto completo]
[Instrucciones detalladas]
"""

USER_PROMPT = """
[Historial de conversaciÃ³n previo]
Usuario: Â¿CuÃ¡l es la arquitectura?
"""
```

### Stage 4: Procesamiento en LLM

```
LLM_INPUT = [{
  "role": "system",
  "content": SYSTEM_PROMPT
}, {
  "role": "user",
  "content": USER_PROMPT
}]

LLM_CONFIG = {
  "model": "gpt-4-turbo",
  "temperature": 0.7,
  "max_tokens": 1000
}

LLM_OUTPUT = {
  "choices": [{
    "message": {
      "content": "La arquitectura estÃ¡ basada en Flux Pattern..."
    }
  }]
}
```

### Stage 5: Formato de Respuesta

```
OUTPUT = {
  "reply": "La arquitectura estÃ¡ basada en Flux Pattern...",
  "timestamp": "2025-12-17T10:30:00.000Z",
  "success": true,
  "model": "gpt-4-turbo"
}
```

---

## ğŸ§  Sistema Inteligente

### Contexto Embedido

El sistema incluye automÃ¡ticamente en cada prompt:

```
NIVEL 1: InformaciÃ³n General
â”œâ”€ Nombre del proyecto
â”œâ”€ DescripciÃ³n
â””â”€ VisiÃ³n general

NIVEL 2: CaracterÃ­sticas
â”œâ”€ Dashboard
â”œâ”€ GestiÃ³n de Flota
â”œâ”€ Monitoreo RT
â””â”€ ... (9 caracterÃ­sticas mÃ¡s)

NIVEL 3: Stack TÃ©cnico
â”œâ”€ React 18.3
â”œâ”€ TypeScript
â”œâ”€ Vite
â”œâ”€ TailwindCSS
â”œâ”€ Supabase
â”œâ”€ PostgreSQL
â””â”€ APIs (Google Maps, GPS)

NIVEL 4: Arquitectura
â”œâ”€ Capas de la aplicaciÃ³n
â”œâ”€ Flux Pattern
â”œâ”€ Componentes
â”œâ”€ Hooks
â”œâ”€ Store
â””â”€ Servicios

NIVEL 5: Ã‰picas
â”œâ”€ GestiÃ³n de Flota
â”œâ”€ GestiÃ³n de Conductores
â”œâ”€ Monitoreo RT
â”œâ”€ PlanificaciÃ³n de Rutas
â”œâ”€ GestiÃ³n de Combustible
â”œâ”€ Mantenimiento Predictivo
â”œâ”€ GestiÃ³n de Incidentes
â”œâ”€ Reportes
â”œâ”€ Integraciones
â””â”€ Seguridad

NIVEL 6: Roles
â”œâ”€ Administrador
â”œâ”€ Gerente
â”œâ”€ Supervisor
â”œâ”€ Planificador
â”œâ”€ Conductor
â”œâ”€ MecÃ¡nico
â””â”€ RRHH

NIVEL 7: Base de Datos
â”œâ”€ Supabase
â”œâ”€ PostgreSQL
â”œâ”€ 8+ tablas principales
â””â”€ RPC Functions
```

### Inteligencia Conversacional

```
ENTRADA 1: "Â¿CuÃ¡l es la arquitectura?"
CONTEXTO: usuario nuevo
RESPUESTA: ExplicaciÃ³n general

ENTRADA 2: "Â¿Me explicas Flux Pattern?" (con histÃ³rico)
CONTEXTO: conversaciÃ³n continua
RESPUESTA: Profundiza en lo anterior

ENTRADA 3: "Dame un ejemplo" (con histÃ³rico completo)
CONTEXTO: usuario entiende conceptos
RESPUESTA: Ejemplo prÃ¡ctico del proyecto

â† El chatbot aprende y profundiza segÃºn el contexto
```

---

## ğŸ’¾ Almacenamiento de Estado

### En n8n (Ephemeral)

```
EXECUTION_MEMORY = {
  nodeInputs: {
    webhook: $json,
    extraer_contexto: {...},
    construir_prompt: {...}
  },
  nodeOutputs: {
    extraer_contexto: {...},
    construir_prompt: {...},
    openai_llm: {...}
  }
}
```

### En Cliente (HistÃ³rico)

```
CLIENT_STORAGE = {
  conversationHistory: [
    {role: "user", content: "..."},
    {role: "assistant", content: "..."},
    {role: "user", content: "..."},
    {role: "assistant", content: "..."}
  ]
}
```

âš ï¸ **Importante**: Guardar histÃ³rico en cliente es responsabilidad de quien consume la API

---

## ğŸ” Seguridad y Credenciales

### JerarquÃ­a de Credenciales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   n8n Credentials Store     â”‚  ğŸ”’ Encriptado
â”‚  (OpenAI/Anthropic keys)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LLM Providers  â”‚  ğŸŒ Externo
        â”‚ (OpenAI/Claude)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- âœ… Keys nunca en JSON
- âœ… Keys nunca en git
- âœ… Keys encriptadas en n8n
- âœ… ComunicaciÃ³n HTTPS

---

## ğŸ“Š Performance y Optimizaciones

### Tiempo de Respuesta

```
Webhook              ~50ms
â”œâ”€ Extraer Contexto  ~10ms (Code node)
â”œâ”€ Construir Prompt  ~20ms (Code node)
â”œâ”€ OpenAI Request    ~2-5 segundos â° (BOTTLENECK)
â”‚  â”œâ”€ Network        ~500ms
â”‚  â”œâ”€ LLM Processing ~1500-4000ms
â”‚  â””â”€ Response       ~500ms
â”œâ”€ Procesar Respuesta ~10ms
â””â”€ Responder         ~10ms

TOTAL: 2-6 segundos (tÃ­picamente)
```

### Optimizaciones Posibles

```
1ï¸âƒ£  Caching
    - Almacenar preguntas frecuentes
    - Respuestas cacheadas: ~50ms

2ï¸âƒ£  Modelo mÃ¡s rÃ¡pido
    - Cambiar a GPT-3.5-turbo: ~1-2 seg
    - O Claude-Instant: ~1-2 seg

3ï¸âƒ£  Contexto menor
    - Reducir maxTokens: 1000 â†’ 500
    - Impacto: ~20% menos lento

4ï¸âƒ£  Batching
    - Procesar mÃºltiples preguntas juntas
    - Reducir overhead de network
```

---

## ğŸ”„ Manejo de Errores

### Error Handling Flow

```
TRY:
  1. Validar entrada JSON
  2. Conectar a LLM
  3. Procesar respuesta
  4. Formatear salida

CATCH ERROR:
  â”œâ”€ API Key invÃ¡lida
  â”‚  â†’ Mensaje: "Error de credenciales"
  â”‚  â†’ Status: 401
  â”‚
  â”œâ”€ Timeout (>30 seg)
  â”‚  â†’ Mensaje: "LLM no respondiÃ³"
  â”‚  â†’ Status: 504
  â”‚
  â”œâ”€ Formato invÃ¡lido
  â”‚  â†’ Mensaje: "Solicitud malformada"
  â”‚  â†’ Status: 400
  â”‚
  â””â”€ Error desconocido
     â†’ Mensaje: "Error interno"
     â†’ Status: 500
```

---

## ğŸ“ˆ Escalabilidad

### Capacidad Actual

```
Requests simultÃ¡neos: ~10-20 (dependiendo de n8n)
Conversaciones activas: Ilimitadas (sin histÃ³rico)
Usuarios concurrentes: 5-10
Requests/dÃ­a: ~1000-2000 (estimado)
Costo/mes: $5-20 USD (OpenAI) o $1-5 USD (Claude)
```

### Para Escalar

```
ğŸš€ Soluciones:
1. MÃ¡s instancias de n8n
2. Load balancer (nginx)
3. CachÃ© Redis para respuestas frecuentes
4. Base de datos para historial persistente
5. Message queue (RabbitMQ) para picos

ğŸ“Š Monitoreo necesario:
- Latencia por nodo
- Uso de tokens
- Tasa de errores
- Costos mensuales
```

---

## ğŸ¯ Casos de Uso

### Caso 1: Pregunta Simple

```
Usuario: "Â¿CuÃ¡l es el stack?"

â†’ ContextualizaciÃ³n: Usuario pregunta sobre tech stack
â†’ LLM procesa: Acceso a PROJECT_CONTEXT
â†’ Respuesta: React, TypeScript, Supabase...
â†’ Tiempo: ~2-3 segundos
```

### Caso 2: ConversaciÃ³n Progresiva

```
M1: "Â¿CuÃ¡l es la arquitectura?"
    â†’ Respuesta general

M2: "Â¿QuÃ© es Flux Pattern?" (con M1 como contexto)
    â†’ Respuesta con referencia a M1

M3: "Â¿DÃ³nde estÃ¡ en el cÃ³digo?" (con M1+M2)
    â†’ Respuesta tÃ©cnica con ubicaciones
```

### Caso 3: Debugging

```
Dev: "Â¿Por quÃ© el mapa es lento?"

â†’ LLM analiza:
  - Estructura del cÃ³digo
  - Patrones usados
  - Posibles problemas

â†’ Respuesta:
  - Causas potenciales
  - Optimizaciones
  - Referencia a cÃ³digo
```

---

## ğŸ”® Futuras Mejoras

### V3.0 Planeada

```
âœ¨ Features Futuras:
â”œâ”€ ğŸ” BÃºsqueda en archivos del repo
â”œâ”€ ğŸ’¾ Persistencia de histÃ³ricos
â”œâ”€ ğŸ¯ Recomendaciones automÃ¡ticas
â”œâ”€ ğŸ“Š Analytics del uso
â”œâ”€ ğŸ¤– Multi-LLM (seleccionar mejor)
â”œâ”€ ğŸ”„ Streaming de respuestas
â””â”€ ğŸŒ Soporte multi-idioma
```

---

## ğŸ“š Referencias

- [n8n Docs](https://docs.n8n.io)
- [OpenAI API](https://platform.openai.com/docs)
- [Claude API](https://docs.anthropic.com)
- [Flux Pattern](https://redux.js.org/understanding/history-and-design/prior-art)

---

**Ãšltima actualizaciÃ³n**: 2025-12-17  
**VersiÃ³n**: 1.0
